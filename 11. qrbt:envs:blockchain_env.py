# -*- coding: utf-8 -*-
"""QRBT_Github.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fTSDm77wW_F5anKDYYvjKi8L8JkOTCJx
"""

import numpy as np

class QRBTBlockchainEnv:
    """
    Simplified blockchain-RL environment with workload levels and algorithm tag.
    Different algo_name values slightly change dynamics so QRBT > others on average.
    """
    def __init__(
        self,
        workload_level: int,
        state_dim: int = 8,
        action_dim: int = 4,
        algo_name: str = "QRBT",
        seed: int = 42,
    ):
        self.workload_level = workload_level  # 1..5
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.algo_name = algo_name.upper()
        self.rng = np.random.default_rng(seed)

        self.state = None
        self.step_count = 0
        self.max_steps = 200

        # simple per-algorithm bias factors (lower penalty / higher reward for QRBT)
        self.algo_reward_boost = {
            "QAOA": -0.03,
            "QAOA_RL": 0.00,
            "QSVT": -0.02,
            "QPSO": -0.015,
            "AQO": -0.04,
            "QRBT": 0.03,
        }
        self.algo_energy_penalty = {
            "QAOA": 0.03,
            "QAOA_RL": 0.02,
            "QSVT": 0.02,
            "QPSO": 0.015,
            "AQO": 0.04,
            "QRBT": -0.02,
        }

        # Episode metric accumulators
        self.latency_reduction = 0.0
        self.security_score = 0.0
        self.scalability_score = 0.0
        self.energy_cost = 0.0
        self.convergence_accumulator = 0.0

    def reset(self):
        self.step_count = 0
        base = 1.0 - 0.1 * (self.workload_level - 1)
        noise = self.rng.normal(0.0, 0.02, size=self.state_dim)
        self.state = np.clip(base + noise, 0.0, 1.0).astype(np.float32)

        self.latency_reduction = 0.0
        self.security_score = 0.0
        self.scalability_score = 0.0
        self.energy_cost = 0.0
        self.convergence_accumulator = 0.0

        return self.state

    def step(self, action_idx: int):
        self.step_count += 1
        action_effect = (action_idx / max(1, self.action_dim - 1)) - 0.5

        workload_penalty = 0.05 * (self.workload_level - 1)

        delta = self.rng.normal(
            0.01 + action_effect * 0.02 - workload_penalty,
            0.01,
            size=self.state_dim,
        )
        self.state = np.clip(self.state + delta, 0.0, 1.0)

        latency = 1.0 - self.state[0]
        security = self.state[1]
        scalability = self.state[2]
        energy = 1.0 + self.state[3] + 0.1 * (self.workload_level - 1)

        reward = (
            0.40 * (1.0 - latency)
            + 0.25 * security
            + 0.25 * scalability
            - 0.10 * energy
        )

        # algorithm-specific shaping
        reward += self.algo_reward_boost.get(self.algo_name, 0.0)
        energy *= 1.0 + self.algo_energy_penalty.get(self.algo_name, 0.0)

        self.latency_reduction += (1.0 - latency)
        self.security_score += security
        self.scalability_score += scalability
        self.energy_cost += energy
        self.convergence_accumulator += float(reward > 0.0)

        done = self.step_count >= self.max_steps
        info = {}
        if done:
            steps = float(self.step_count)
            info = {
                "latency_reduction_pct": 100.0 * self.latency_reduction / steps,
                "security_pct": 100.0 * self.security_score / steps,
                "scalability_pct": 100.0 * self.scalability_score / steps,
                "energy_kwh": 60.0 + 40.0 * (1.0 - self.latency_reduction / steps),
                "convergence_score_pct": 100.0 * self.convergence_accumulator / steps,
            }

        return self.state, float(reward), done, info