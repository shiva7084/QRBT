# -*- coding: utf-8 -*-
"""QRBT_Github.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fTSDm77wW_F5anKDYYvjKi8L8JkOTCJx
"""

import os
import json
from pathlib import Path
import yaml
import torch
import torch.nn.functional as F
from tqdm import trange

!pip install qrbt

from qrbt.envs.blockchain_env import QRBTBlockchainEnv
from qrbt.rl.actor_critic import ActorCritic
from qrbt.rl.buffers import RolloutBuffer
from qrbt.metrics.evaluation import ResultsAggregator


def train_level(level: int, cfg, device):
    env = QRBTBlockchainEnv(workload_level=level,
                            state_dim=cfg["env"]["state_dim"],
                            action_dim=cfg["env"]["action_dim"])
    model = ActorCritic(cfg["env"]["state_dim"], cfg["env"]["action_dim"]).to(device)
    optimizer = torch.optim.Adam([
        {"params": model.actor_head.parameters(), "lr": cfg["training"]["actor_lr"]},
        {"params": model.critic_head.parameters(), "lr": cfg["training"]["critic_lr"]},
        {"params": list(model.fc1.parameters()) +
                 list(model.fc2.parameters()) +
                 list(model.fc3.parameters()), "lr": cfg["training"]["actor_lr"]},
    ])

    buffer = RolloutBuffer()
    aggregator = ResultsAggregator()

    total_steps = cfg["training"]["total_steps"]
    gamma = cfg["training"]["gamma"]
    batch_size = cfg["training"]["batch_size"]

    state = torch.tensor(env.reset(), dtype=torch.float32, device=device).unsqueeze(0)

    for step in trange(total_steps, desc=f"Level {level}"):
        action, log_prob, value = model.act(state)
        next_state_np, reward, done, info = env.step(int(action.item()))
        reward_t = torch.tensor([reward], dtype=torch.float32, device=device)

        buffer.add(state.squeeze(0), action, log_prob, reward_t, done, value)

        if done and info:
            aggregator.add_episode_info(level, info)
            next_state_np = env.reset()

        state = torch.tensor(next_state_np, dtype=torch.float32, device=device).unsqueeze(0)

        # Update every batch_size steps
        if (step + 1) % batch_size == 0:
            states, actions, log_probs_old, rewards, dones, values = buffer.to_tensors(device)

            # Compute returns (simple discounted)
            returns = []
            G = 0.0
            for r, d in zip(reversed(rewards), reversed(dones)):
                G = r + gamma * G * (1.0 - d)
                returns.insert(0, G)
            returns = torch.stack(returns).detach()

            advantages = returns - values.detach()

            log_probs, entropy, values_new = model.evaluate_actions(states, actions)
            ratio = torch.exp(log_probs - log_probs_old.detach())

            # Simple PPO-style clipped loss
            clip_eps = 0.2
            obj1 = ratio * advantages
            obj2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advantages
            actor_loss = -torch.min(obj1, obj2).mean()
            critic_loss = F.mse_loss(values_new.squeeze(-1), returns)
            entropy_loss = -entropy.mean()

            loss = actor_loss + 0.5 * critic_loss + 0.01 * entropy_loss

            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
            optimizer.step()

            buffer.clear()

    return aggregator.aggregate_by_level()[level]


def main():
    with open("config.yaml", "r") as f:
        cfg = yaml.safe_load(f)

    device = torch.device(cfg["training"]["device"] if torch.cuda.is_available() else "cpu")
    out_dir = Path(cfg["results"]["output_dir"])
    out_dir.mkdir(parents=True, exist_ok=True)

    all_levels_summary = {}
    for lvl in cfg["env"]["workload_levels"]:
        summary = train_level(lvl, cfg, device)
        all_levels_summary[lvl] = summary

    with open(out_dir / "qrbt_results.json", "w") as f:
        json.dump(all_levels_summary, f, indent=2)

    print("Saved results to", out_dir / "qrbt_results.json")


if __name__ == "__main__":
    main()