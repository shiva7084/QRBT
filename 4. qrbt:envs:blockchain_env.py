# -*- coding: utf-8 -*-
"""QRBT_Github.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fTSDm77wW_F5anKDYYvjKi8L8JkOTCJx
"""

import numpy as np

class QRBTBlockchainEnv:
    """
    Simplified blockchain-RL environment with workload levels.
    State encodes: latency, tps, mempool, energy, security, etc. (normalized).
    """
    def __init__(self, workload_level: int, state_dim: int = 8, action_dim: int = 4, seed: int = 42):
        self.workload_level = workload_level  # 1..5
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.rng = np.random.default_rng(seed)

        self.state = None
        self.step_count = 0
        self.max_steps = 200

        # Internal trackers for metrics (toy but deterministic-ish)
        self.latency_reduction = 0.0
        self.security_score = 0.0
        self.scalability_score = 0.0
        self.energy_cost = 0.0
        self.convergence_accumulator = 0.0

    def reset(self):
        self.step_count = 0
        # Initial state: slightly worse for higher workloads
        base = 1.0 - 0.1 * (self.workload_level - 1)
        noise = self.rng.normal(0.0, 0.02, size=self.state_dim)
        self.state = np.clip(base + noise, 0.0, 1.0).astype(np.float32)

        self.latency_reduction = 0.0
        self.security_score = 0.0
        self.scalability_score = 0.0
        self.energy_cost = 0.0
        self.convergence_accumulator = 0.0

        return self.state

    def step(self, action_idx: int):
        """
        action_idx in [0, action_dim-1]; you can later map to circuit depth,
        validator set size, fee policy, etc.
        """
        self.step_count += 1

        # Map action index to a small continuous control vector
        action_effect = (action_idx / max(1, self.action_dim - 1)) - 0.5

        # Workload penalty increases with level
        workload_penalty = 0.05 * (self.workload_level - 1)

        # Toy dynamics: update state with some improvement/noise
        delta = self.rng.normal(0.01 + action_effect * 0.02 - workload_penalty,
                                0.01, size=self.state_dim)
        self.state = np.clip(self.state + delta, 0.0, 1.0)

        # Derive proxy metrics from state
        latency = 1.0 - self.state[0]          # lower is better
        security = self.state[1]               # higher is better
        scalability = self.state[2]            # higher is better
        energy = 1.0 + self.state[3] + 0.1 * (self.workload_level - 1)  # higher is worse

        # Reward combines security, scalability, latency reduction, energy
        reward = (
            0.40 * (1.0 - latency) +      # latency reduction
            0.25 * security +
            0.25 * scalability -
            0.10 * energy
        )

        # Accumulate episode metrics
        self.latency_reduction += (1.0 - latency)
        self.security_score += security
        self.scalability_score += scalability
        self.energy_cost += energy
        self.convergence_accumulator += float(reward > 0.0)

        done = self.step_count >= self.max_steps

        info = {}
        if done:
            # Average over episode; scale to percentage-like output
            steps = float(self.step_count)
            info = {
                "latency_reduction_pct": 100.0 * self.latency_reduction / steps,
                "security_pct": 100.0 * self.security_score / steps,
                "scalability_pct": 100.0 * self.scalability_score / steps,
                "energy_kwh": 60.0 + 40.0 * (1.0 - self.latency_reduction / steps),  # toy
                "convergence_score_pct": 100.0 * self.convergence_accumulator / steps,
            }

        return self.state, float(reward), done, info